"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[61303],{92330:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>r,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var n=a(87462),l=(a(67294),a(3905));const i={sidebar_position:1},o="Deploy llama-2 on AWS",s={unversionedId:"tutorials/llama2-on-aws",id:"version-v0.5/tutorials/llama2-on-aws",title:"Deploy llama-2 on AWS",description:"This tutorial demonstrates how to deploy llama-2 using Walrus on AWS with CPU, and utilize it through a user-friendly web UI.",source:"@site/versioned_docs/version-v0.5/tutorials/llama2-on-aws.md",sourceDirName:"tutorials",slug:"/tutorials/llama2-on-aws",permalink:"/docs/v0.5/tutorials/llama2-on-aws",draft:!1,editUrl:"https://github.com/seal-io/docs/edit/main/versioned_docs/version-v0.5/tutorials/llama2-on-aws.md",tags:[],version:"v0.5",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"docs",previous:{title:"Create a Catalog on GitHub",permalink:"/docs/v0.5/tutorials/catalog-on-github"},next:{title:"Integration with CI/CD Tools",permalink:"/docs/v0.5/tutorials/integrate-with-cicd"}},r={},d=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"The Simple Way",id:"the-simple-way",level:2},{value:"Add the llama-2 Service Template",id:"add-the-llama-2-service-template",level:3},{value:"Configure Environment and AWS Credentials",id:"configure-environment-and-aws-credentials",level:3},{value:"Create the llama-2 Service",id:"create-the-llama-2-service",level:3},{value:"Accessing the llama-2 Web UI",id:"accessing-the-llama-2-web-ui",level:3},{value:"Deep Dive: Building the llama-2 Image from Scratch",id:"deep-dive-building-the-llama-2-image-from-scratch",level:2}],c={toc:d};function m(e){let{components:t,...i}=e;return(0,l.kt)("wrapper",(0,n.Z)({},c,i,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"deploy-llama-2-on-aws"},"Deploy llama-2 on AWS"),(0,l.kt)("p",null,"This tutorial demonstrates how to deploy llama-2 using Walrus on AWS with CPU, and utilize it through a user-friendly web UI."),(0,l.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,l.kt)("p",null,"To follow this tutorial, you will need:"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"An AWS account with associated ",(0,l.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html"},"credentials"),", and sufficient permissions to create EC2 instances."),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("a",{parentName:"li",href:"/deploy/standalone"},"Walrus installed"),".")),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"Note:\nWhile using CPU is cheaper than GPU, it still incurs costs corresponding to the EC2 instance.")),(0,l.kt)("h2",{id:"the-simple-way"},"The Simple Way"),(0,l.kt)("p",null,"With Walrus, you can have a running llama-2 instance on AWS with a user-friendly web UI in about a minute. Just follow these steps:"),(0,l.kt)("h3",{id:"add-the-llama-2-service-template"},"Add the llama-2 Service Template"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"Log in to Walrus, click on ",(0,l.kt)("inlineCode",{parentName:"li"},"Operations Center")," in the left navigation, go to the ",(0,l.kt)("inlineCode",{parentName:"li"},"Templates")," tab, and click the ",(0,l.kt)("inlineCode",{parentName:"li"},"New Template")," button."),(0,l.kt)("li",{parentName:"ol"},"Enter a template name, e.g., ",(0,l.kt)("inlineCode",{parentName:"li"},"llama-2"),"."),(0,l.kt)("li",{parentName:"ol"},"In the source field, enter ",(0,l.kt)("inlineCode",{parentName:"li"},"https://github.com/walrus-tutorials/llama2-on-aws"),"."),(0,l.kt)("li",{parentName:"ol"},"Click ",(0,l.kt)("inlineCode",{parentName:"li"},"Save"),".")),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"llama2-add-template",src:a(7499).Z,width:"4368",height:"2274"})),(0,l.kt)("h3",{id:"configure-environment-and-aws-credentials"},"Configure Environment and AWS Credentials"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"In the left navigation, click on ",(0,l.kt)("inlineCode",{parentName:"li"},"Application Management"),", go to the ",(0,l.kt)("inlineCode",{parentName:"li"},"default")," project view, and click the ",(0,l.kt)("inlineCode",{parentName:"li"},"Connectors")," tab."),(0,l.kt)("li",{parentName:"ol"},"Click the ",(0,l.kt)("inlineCode",{parentName:"li"},"New Connector")," button and select the ",(0,l.kt)("inlineCode",{parentName:"li"},"Cloud Provider")," type."),(0,l.kt)("li",{parentName:"ol"},"Enter a connector name, e.g., ",(0,l.kt)("inlineCode",{parentName:"li"},"aws"),"."),(0,l.kt)("li",{parentName:"ol"},"Choose ",(0,l.kt)("inlineCode",{parentName:"li"},"AWS")," for the ",(0,l.kt)("inlineCode",{parentName:"li"},"Type")," option."),(0,l.kt)("li",{parentName:"ol"},"Select ",(0,l.kt)("inlineCode",{parentName:"li"},"Tokyo (ap-northeast-1)")," for the ",(0,l.kt)("inlineCode",{parentName:"li"},"Region")," option."),(0,l.kt)("li",{parentName:"ol"},"Click ",(0,l.kt)("inlineCode",{parentName:"li"},"Save"),".")),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"Note:\nThe specified region is used here because the subsequent steps involve using an AMI from that region. If you want to use a different region, you can export the AMI to your region or refer to the following sections on how to build the llama-2 image from scratch.")),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"llama2-add-connector",src:a(31980).Z,width:"4370",height:"2276"})),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"Click the ",(0,l.kt)("inlineCode",{parentName:"li"},"Environments")," tab, click the ",(0,l.kt)("inlineCode",{parentName:"li"},"New Environment")," button."),(0,l.kt)("li",{parentName:"ol"},"Enter an environment name, e.g., ",(0,l.kt)("inlineCode",{parentName:"li"},"dev"),"."),(0,l.kt)("li",{parentName:"ol"},"Click the ",(0,l.kt)("inlineCode",{parentName:"li"},"Add Connector")," button and select the ",(0,l.kt)("inlineCode",{parentName:"li"},"aws")," connector created in the previous step."),(0,l.kt)("li",{parentName:"ol"},"Click ",(0,l.kt)("inlineCode",{parentName:"li"},"Save"),".")),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"llama2-add-environment",src:a(42659).Z,width:"4366",height:"2280"})),(0,l.kt)("h3",{id:"create-the-llama-2-service"},"Create the llama-2 Service"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"In the ",(0,l.kt)("inlineCode",{parentName:"li"},"Environments")," tab, click on the name of the ",(0,l.kt)("inlineCode",{parentName:"li"},"dev")," environment to enter its view."),(0,l.kt)("li",{parentName:"ol"},"Click the ",(0,l.kt)("inlineCode",{parentName:"li"},"New Service")," button."),(0,l.kt)("li",{parentName:"ol"},"Enter a service name, e.g., ",(0,l.kt)("inlineCode",{parentName:"li"},"my-llama-2"),"."),(0,l.kt)("li",{parentName:"ol"},"Choose ",(0,l.kt)("inlineCode",{parentName:"li"},"llama-2")," in the ",(0,l.kt)("inlineCode",{parentName:"li"},"Template")," option."),(0,l.kt)("li",{parentName:"ol"},"Click ",(0,l.kt)("inlineCode",{parentName:"li"},"Save"),".")),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"Note:\nThe default service configuration assumes your AWS account has a default VPC in the corresponding region. If you don't have a default VPC, create a new VPC, associate a subnet and a security group with it in the AWS VPC console.\nThe security group needs to open port 7860 TCP (for accessing the llama-2 web UI). You can set your VPC name and security group name in the service configuration.")),(0,l.kt)("h3",{id:"accessing-the-llama-2-web-ui"},"Accessing the llama-2 Web UI"),(0,l.kt)("p",null,"You can see the deployment and running status of the llama-2 service on its details page. Once the llama-2 service deployment is complete, you can access its web UI by clicking the access link of the service in the Walrus UI."),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"llama2-service-detail",src:a(44430).Z,width:"3686",height:"2284"})),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"llama2-webui",src:a(97971).Z,width:"3538",height:"1840"})),(0,l.kt)("h2",{id:"deep-dive-building-the-llama-2-image-from-scratch"},"Deep Dive: Building the llama-2 Image from Scratch"),(0,l.kt)("p",null,"The above instructions utilized a pre-built llama-2 image. This approach saves time as you don't need to download the large language model (often with a significant file size) or build the inference service when creating a new llama-2 instance.\nThis section explains how such a llama-2 image is built."),(0,l.kt)("p",null,"You can find the complete build process ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/walrus-tutorials/llama2-on-aws/blob/build/main.tf"},"here"),"."),(0,l.kt)("p",null,"Key steps include:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"# get text-generation-webui\ngit clone https://github.com/oobabooga/text-generation-webui && cd text-generation-webui\n# configure text-generation-webui\nln -s docker/{Dockerfile,docker-compose.yml,.dockerignore} .\ncp docker/.env.example .env\nsed -i '/^CLI_ARGS=/s/.*/CLI_ARGS=--model llama-2-7b-chat.ggmlv3.q4_K_M.bin --wbits 4 --listen --auto-devices/' .env\nsed -i '/^\\s*deploy:/,$d' docker/docker-compose.yml\n# get quantized llama-2\ncurl -L https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_K_M.bin --output ./models/llama-2-7b-chat.ggmlv3.q4_K_M.bin\n# build and run\ndocker compose up --build\n")),(0,l.kt)("p",null,"In essence, this process downloads the quantized llama-2-7b-chat model, then builds and utilizes text-generation-webui to launch the llama-2 service."))}m.isMDXComponent=!0},31980:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/llama2-add-connector-c0383dcddad20847aac31617900f58f3.png"},42659:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/llama2-add-env-5c0e81c9c8bb90a396f25e97a8dd1637.png"},7499:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/llama2-add-template-c3508d92b47c493dad7d87ca070fdcb4.png"},44430:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/llama2-service-detail-dc581da4686b4c0d0561d1a21a6fe065.png"},97971:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/llama2-web-ui-1d82a84f88f74bd6e3add858afa5e0b8.png"}}]);